---
title: "[BasicML] XGBoost"
excerpt: "XGBoost,부스팅"
categories:
    - BasicML

tag:
    - python
    - machine learning

author_profile: true    #작성자 프로필 출력 여부

toc: true   #Table Of Contents 목차 
toc_sticky: true
---
## 
이 문서에 나오는 자료와 데이터는 권철민 저의 머신러닝 완벽가이드와 인프런의 강의를 바탕으로 정리한 문서입니다.

## Intro
Kaggle과 같은 대회에서 많이 활용되면서 분석가들에게 사용되기 시작된 모델인 XGBoost를 알아보자.

##
XGboost의 라이브러리는 C/C++로 작성이 되어 있기 때문에, 최초에는 호환되지 않던 파이썬 또는 R 사용자를 위한 API를 제공하였다. 

또 많은 사이킷런 기반의 머신러닝을 사용하는 유저를 위해 사이킷런 패키지에서 XGBoost를 사용할 수 있도록 래퍼 클래스(warpper class)를 제공하였고, 파이썬 래퍼와 사이킷런 래퍼 두 가지 방법으로 XGBoost를 사용할 수 있게 된것이다.


## 주요 파라미터

|파이썬 Wrapper|사이킷런 Wrapper |설명|
|----|----|----|
|eta|learning_rate| 0에서 1사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값|
|num_boost_rounds|n_estimators|약한 학습기의 개수(반복 수행 회수)|
|min_child_weight|min_child_weight|결정트리의 min_child_leaf와 유사, 과적합 조절용으로 특정 weight가 넘으면 child를 만들것인지 결정하는 파라미터|
|max_depth|max_depth|결정트리의 max_depth와 동일, 트리의 최대 깊이|
|sub_sample|subsample|트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정하는 파라미터|
|lambda|reg_lambda|L2 규제(Regularization) 적용 값|
|alpha|reg_alpha|L1 규제(Regularization) 적용 값|
|colsample_bytree|colsample_bytree|트리 생성에 필요한 컬럼을 임의로 샘플링 하는 데 사용|
|scale_pos_weight|sacle_pos_weight|특정 값으로 치우친 비대칭한 클래스(특정 분류 클래스가 너무 많은 경우)의 데이터 세트 균형을 유지하기 위한 파라미터|
|gamma|gamma|해당 값보다 큰 손실(loss)가 감소된 경우에 리프 노드를 분리한다|


## 조기 중단 기능

XGBoost는 특정 반복 횟수 만큼 더 이상 비용함수가 감소되지 않으면 지정된 반복횟수를 다 완료하지 않고 수행을 종료할 수 있다.

![image](https://user-images.githubusercontent.com/81638919/147891312-c729ac0e-4493-4bf8-bf21-4a00c6d75a1f.png)

실제 train 데이터를 학습을 하게 되면, 주황색으로 표시된 Trainning error를 줄이는데만 몰두를 하게 된다. 

이렇게 과적합된 모델을 Validation(검증 데이터)에 적용하면, 초록색으로 표시된 그래프와 같이 어느 정도 손실이 줄지만 어느 순간이 되면 과적합이 되어 error가 많아지게 된다.
따라서 Early Stopping은 어느 수준으로 오류를 줄이는데 성공하면, 수행을 종료하여 학습을 위한 시간을 단축 시키고 Train 데이터에 과적합을 방지할 수 있다.


|주요 파라미터|설명|
|----|----|
|early_stopping_rounds|더 이상 비용 평가 지표가 감소하지 않는 최대 반복횟수|
|eval_metric|반복 수행 시 사용하는 비용 평가 지표|
|eval_set|평가를 수행하는 별도의 검증 데이터 세트. 일반적으로 검증 데이터 세트에서 반복적으로 비용 감소 성능 평가|


하지만 너무 반복 횟수(early_stopping_rounds)를 단축할 경우 예측 성능 최적화가 안된 상태에서 학습이 종료 될 수 있으므로 유의가 필요하다.
