---
title: "[딥러닝 이해] 02 선형대수 기초"
excerpt: "선형결합,기약 사다리꼴,랭크, 가역 "
categories:
    - DeepLearning

tag:
    - 기초 수학

author_profile: true    #작성자 프로필 출력 여부

toc: true   #Table Of Contents 목차 
toc_sticky: true
---

```
딥러닝을 공부하며 필요한 수학 / 딥러닝 모델들을 정리할 예정입니다. 
학습하고 있는 내용이기에 오류가 있을 수 있습니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 남겨주세요 😄
```
Deep Learning - 딥러닝을 위한 기초 수학

## Linear Combination

- 선형결합은 아래와 같이 V1,V2,Vn의 벡터가 주어졌을 때,각각의 계수 또는 가중치를 곱해주고, 모두 합친 형태를 선형 결합이라고 한다.

  ![image](https://user-images.githubusercontent.com/81638919/163416552-be3e0a7c-386e-47c0-b145-3a3b31d88115.png)



- 위의 선형결합의 정의에 의해서 아래와 같이 표현할 수 있음 

  ![image](https://user-images.githubusercontent.com/81638919/163416590-fad59d0e-e538-4388-b136-37d58f24fe23.png)

- 앞으로 배울 단층 퍼셉트론에서 활성화 함수에 넣기 전에 선형 결합을 먼저 진행함.
- 이렇게 어떤 입력 Input x1에 w1을 곱하고, x2에 w1를 곱하여, 하나의 식으로 표현한 것 또한 선형 결합의 형태이다. 아래 그림은 h = x1w1+x2w2의 형태가 될 것이다.

   ![image](https://user-images.githubusercontent.com/81638919/163416619-9473e5bd-bec9-445a-8d0f-a04a30bfb9a9.png)

- Span : 선형 결합에서 얻는 방정식을 통해 형성할 수 있는 공간이 있는데 이를 Span이라고 함. 즉, 주어진 벡터들의 선형결합으로 표현되는 모든 벡터들의 집합

  ![image](https://user-images.githubusercontent.com/81638919/163416757-a12d2593-3fcb-4487-a492-eda6ad5fe231.png)

- v1과 v2라는 2개의 벡터를 선형결합하면, 다음과 같은 평면을 얻을 수 있음.

    ![image](https://user-images.githubusercontent.com/81638919/163416725-1bf15627-4342-468f-bc9b-872431c4155d.png)

출처 :**인공지능을 위한 선형대수**

- Linear independent

  ![image](https://user-images.githubusercontent.com/81638919/163416799-59fa35a6-be46-49e0-840b-df47e4be471b.png)

  만약 0벡터라면 선형종속. 그 이유는 c0=0을 만족하는 많은  c가 존재하기 때문, 반면에 0 벡터가 아닌 벡터는 선형독립 왜냐하면 cx=0을 만족하는 c는 0 밖에 없기 때문임.

## Row reduction

- Row reduction

  선형 시스템의 해를 얻기 위해 선형 시스템을 기약 사다리꼴(reduced echelon form)으로 만든다면, 쉽게 해를 찾을 수 있음. 어떠한 행렬을 기약 사다리꼴로 만들기 위해 행줄임 (row reduction)을 진행함

    ![image](https://user-images.githubusercontent.com/81638919/163416824-afe1328d-6b71-4334-a084-209776700905.png)

- Row-echelon form

  - non-zero rows가 있다면 matrix의 가장 아래에 위치함

  - the first nonzero entries( 왼쪽에서 처음으로 0이 아닌 entry) 는 위에 있는 first nonzero entires보다 오른쪽에 있음

   
    ![image](https://user-images.githubusercontent.com/81638919/163416884-2059a541-2cb7-4551-b0d8-b3c4ba285eab.png)

  - 여기서 첫번째 행렬만이 Row-echelon form임. 두번째 행렬은 non-zero rows가 가장 아래에 위치하지 않고, 세번째는 세번째 row의 first non-zero entry가 오른쪽에 위치하고 있지 않음.  

- Pivot

  - 위에서 말한 3개의 Row reduction 방법으로, Row-echelon form을 만들 수 있다. 이제 echelon form에서 reduced echelon from을 어떻게 만드는지 알아보자.

  - pivot의 정의는 다음과 같음

    ![image](https://user-images.githubusercontent.com/81638919/163417062-17de6b61-245c-4d5c-9143-6d42f6f19812.png)

  - 즉, row-echelon form에서 각 row의 first non-zero entry를 pivot이라고 함. 그리고 pivot이 나타난 columns을 pivot columns이라고 함

    ![image](https://user-images.githubusercontent.com/81638919/163416953-e0672d7e-0740-4161-99b9-8cb4521cff7b.png)

- Reduced row-echelon form

  - Reduced row-echelon form은 row echelon form에서 Pivot이 1이고 Pivot columns의 나머지 값이 모두 0인 형태를 말함

    ![image](https://user-images.githubusercontent.com/81638919/163416936-2eb3663c-14bf-4c80-b405-cd38c9a18ca9.png)

## Rank 

- Reduced row-echelon-form에서 Pivot의 개수를 Rank라고 함. 하지만 Rank 랭크는 이렇게 한 줄로 정의하기에는 정의가 다양하기 때문에, 여기서는 Pivot의 개수로 생각하자.

- NxN 매트릭스에서 n보다 작은 Rank의 개수를 가지고 있으면, 해당 행렬은 invertable하지 않음

##  The Invertible Matrix Theorem

   ![image](https://user-images.githubusercontent.com/81638919/163417204-cb856507-d402-4be2-90e2-3fe199677ba2.png)

만약에 A가 invertible이면 위의 모든 조건을 다 만족하고, not invertible이면 모든 조건을 만족하지 않음.
