---
title: "[ML] 분류 성능 평가 지표（分類器評估方法）"
excerpt: "정확도, 오차행렬, 정밀도, 재현율, F1스코어, ROC AUC 分類器評估方法"
categories:
    - python
    - machine learning

tag:
    - python
    - machine learning

author_profile: true    #작성자 프로필 출력 여부

toc: true   #Table Of Contents 목차 
toc_sticky: true
---
## Intro

분류기를 만들면, 분류기의 성능을 측정해야하는데 일반적으로는 실제 결과 데이터와 예측 결과 데이터가 얼마나 정확하고 오류가 적게 발생하는가에 기반하지만 단순히 정확도만 가지고 판단했을 경우에는 함정에 빠질 수 있으니 유의해야한다. 

## 정확도의 함정

정확도는 전체 예측한 데이터 건수중에 예측이 정확히 된 데이터 건수를 비율(%)로 나타낸다. 이러한 정확도는 직관적으로 모델 예측 성능을 나타내는 지표이긴 하나, 데이터셋의 구성에 따라 분류기의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하지 않는다.

특히, 분류의 결정값(Target) 데이터들이 불균형한 데이터 일때, 정확도는 정확하지 않다.  예를 들어, 신용카드 사기 검출에 전체 데이터 세트가 10000건이 있는데, 1%만 사기라고 생각하자. 이러한 경우 복잡한 알고리즘 필요없이 단순히 "모두 정상이라고 예측"을 했을 때, 매우 높은 정확도인 99%가 나올 것이다. 이처럼 정확도 평가 지표는 불균형한 결정값(Target)을 가지고 있는 데이터 세트에서는 성능 수치로 사용해서는 안된다. 따라서 정확도의 한계점을 극복하기 위해 여러 가지 분류 지표와 함께 ML모델을 평가해야 한다.

## 오차 행렬(Confustion Matrix)

오차 행렬은 이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표이다.

![image](https://user-images.githubusercontent.com/81638919/133916601-c1f4f9d5-6548-4522-a3d2-9cad49bd3f8a.png)

위 그림에서 
TN는 예측을 Negative(0)로 했으며, 실제도 Negative(0)
FP는 예측을 Positive(1)로 예측을 했지만 실제는 Negative(0)
FN은 예측을 Negative(0)로 했지만 실제는 Positive(1)
TP는 예측을 Positive로 했지만 실제도 Positive라는 의미이다.

이러한 오차 행렬은 sklearn의 metrics 패키지에서 confusion_matrix로 간단히 호출하면 된다.

```python
from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0, 0]

confusion_matrix(y_true, y_pred)
```
```python
array([[2, 0],
       [2, 2]], dtype=int64)
```
이렇게 출력된 오차 행렬은 ndarray형태로 나오며, TN은 array[0,0]로 2, FP는[0,1]로 0, FN은 [1,0]으로 2, TP는 [1,1]로 2에 해당한다. 
풀어서 설명하면, TN은 6개의 데이터 중 Negative 0으로 예측해서 True가 된 결과 2건, FP는 Positive 1로 예측했지만 실제값은 Negative 0인 값이 없으므로 0건 FN은 Negative 0으로 예측했지만 1인 2건, TP는 Positive로 예측했지만 1인값 2건이다. 그리고 일반적으로 불균형한 레이블 클래스를 가지는 이진 분류 모델에서는 중점적으로 찾아야하는 매우 적은 수의 결과값에 Positivie를 설정해 1값을 부여하고, 그렇지 않은 경우 0값을 부여하는 경우가 많다. 예를들어, 사기 행위 예측 모델에서는 사기 행위가 positive 양성으로 1, 정상 행위가 Negative 음성으로 0 값이 결정 값으로 할당되는 것 처럼 말이다.

이제는 오차 행렬에서 파생되는 다양한 지표를 살펴보자.

## 정밀도(Precision), 재현율(Recall)
위에서 신용카드 사기 검출에서 정확도가 왜 제대로 평가 지표로 활용되지 못하는지 말했는데, 불균형한 데이터 세트에서 정확도보다 더 선호되는 평가 지표인 정밀도(Precision)와 재현율(Recall)에 대해서 알아보자.

첫번째로 정밀도(Precision) (TP/(FP+TP))는 예측을 Positive로 한 대상 중(FP+TP)에 예측과 실제값이 Positive로 일치한 데이터의 비율을 뜻하며, 그리고 재현율(Recall) TP /(FN+TP)은 실제 값이 Positive인 대상 중(FN+TP)에 예측과 실제값이 Positive로 일치한 데이터의 비율을 뜻한다.

두 개념이 조금 헷갈리는데, 차이점은 정밀도는 얼마나 정밀하게 Positive를 예측했냐를 나타내는 지표이며, 재현율은 실제 참 중에서 분류기가 참으로 분류한 비율이다. 둘다 TP
분모가 FP + TP / FN + TP 정밀도는 FP(예측을 Positive로 했는데, 실제는 Negative)가 낮아져야 정밀도가 높아지며, 재현율은 FN(예측을 Negative로 했는데, 실제 값이 Positive)의 값이 낮아져야 한다. 즉, 실제 Positive인 클래스를 다르게 예측하면 안된다는 말이다. 그게 재현율의 수치를 높이는 것이다.

## 업무에 따른 재현율과 정밀도의 상대적 중요도
정밀도와 재현율 두가지 지표에 대해서 살펴봤는데, 두 가지의 중요성이 각 비즈니스의 특성에 따라 달라질 수 있다.

재현율은 실제 Positive 중에서 분류기가 Positive로 분류한 비율인데, 재현율이 상대적으로 더 중요한 지표의 경우는 실제 Positive 양성인데 데이터 예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우가 있을 수 있다. 예를 들어, 암 진단 같은 경우 실제로 암인데 음성으로 예측을 해버리면 환자의 생명을 위독하게 만드는 경우가 발생할 수 있다. 또
금융사기 판별을 하는데, 금융사기가 아닌 음성인 데이터라고 판단을 해버리면 회사 입장에서 큰 손해가 발생할 수 있다.

정밀도는 얼마나 정말하게 Positive를 예측했냐를 나타내는 지표이며, 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우 상대적으로 중요하다. 예를 들어, 스팸메일을 분류하는 분류기의 경우 실제 Positive인 스팸 메일을 Negative인 일반 메일로 분류하더라도 사용자가 불편함을 느끼는 정도이지만, 실제 Negative인 일반 메일을 Positive인 스팸 메일로 분류할 경우에는 메일을 아예 받지 못하게 되어 업무에 차질을 생길 수 있다.

## 정밀도/재현율 트레이드 오프

분류하려는 업무의 특성상 

## 정밀도와 재현율의 함정
정밀도와 재현율은 정확도와 비슷하게 숫자놀음이 될 수 있는데,

정밀도를 100%로 만드는 방법은 확실한 기준이 되는 경우만 Positive로 예측하고 나머지는 모두 Negative로 예측

재현율을 100%로 만드는 방법은 모든 환자를 Positive로 예측하면 된다. 재현율은 TP/(TP+FN) 이므로 전체 환자 1000명을 다 Positive로 예측하는 것이다. 이 중 실제 양성인 사람이 30명 정도라도 TN이 수치에 포함되지 않고 FN은 아예 0이므로 30/(30+0)으로 100%가 된다. 따라서 F1 스코어가 나오는데 이를 알아보자.

## F1 Score

F1 스코어(F1 Score)는 정밀도와 재현율을 결합한 지표인데, F1 스코어는 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. 

F1 스코어의 공식은 다음과 같다.

## ROC 곡선(Receiver Operation Characteristic Curve)과 AUC 

분류의 성능 지표로 사용되는 것은 ROC 곡선 면적에 기반한 AUC 값으로 결정한다.
TPR은 True Positive Rate의 약자이며, 이는 재현율(Recall)을 나타낸다. 따라서 TPR은 TP/(FN+TP)이며, 민감도로도 불린다.
FRP은 실제 Negative(음성)을 잘못 예측한 비율을 나타낸다. 즉, 실제는 Negative인데 Positive 또는 Negative로 예측한 것 중 Positive로 잘못 예측한 비율이다.
FRP = FP(실제 음성을 양성으로 예측) /(FP+TN - 실제 음성)


